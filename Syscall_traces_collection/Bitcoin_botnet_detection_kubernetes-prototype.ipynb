{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin deployment created.\n",
      "Deep learning depoyment created. \n",
      "Waiting for 30 sec to let workload run.\n",
      "Containers scanned are :  {'fe26b8b8e242': 'k8s_monero-cpuminer-deployment_monero-cpuminer-deployment-bb5fb4dd9-kn92t_default_9b32d7ef-a791-11e8-a186-025271d58932_0', '5c368c1f7c4f': 'k8s_deep-learning-dynamic_deeplearning-deployment-7c7c877959-st4b9_default_9b7bdf87-a791-11e8-a186-025271d58932_0'}\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import pdb\n",
    "\n",
    "def run_linux_command(command):\n",
    "    p = subprocess.Popen(command, universal_newlines=True, shell=True, \n",
    "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    text = p.stdout.read()\n",
    "    retcode = p.wait()\n",
    "    return text\n",
    "\n",
    "def remove_kubernetes_administrative_containers(dict_containers_nameid):\n",
    "    copy_dict_containers_nameid = dict_containers_nameid.copy()\n",
    "    kubernetes_aministrative_containers = ['k8s_POD', 'k8s_sidecar', 'k8s_dnsmasq', 'k8s_kubernetes', 'k8s_storage-provisioner', 'k8s_kube', 'k8s_etcd']\n",
    "    for container_name in kubernetes_aministrative_containers:\n",
    "        for container_id in dict_containers_nameid.keys():\n",
    "            if dict_containers_nameid[container_id].startswith(container_name):\n",
    "                try:\n",
    "                    del copy_dict_containers_nameid[container_id]\n",
    "                except:\n",
    "                    pass\n",
    "    return copy_dict_containers_nameid\n",
    "\n",
    "def get_container_names(path):\n",
    "    import yaml\n",
    "    filenames = os.listdir(path)\n",
    "    filenames = [filename for filename in filenames if filename.endswith('.yml')]\n",
    "    container_created_name = []\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r') as stream:\n",
    "            try:\n",
    "                yaml_content = (yaml.load(stream))\n",
    "                if yaml_content['kind'] == 'Deployment':\n",
    "                    container_created_name.append(yaml_content['spec']['template']['spec']['containers'][0]['name'])\n",
    "            except yaml.YAMLError as exc:\n",
    "                pass #print(exc)\n",
    "\n",
    "    return(container_created_name)\n",
    "\n",
    "def containerid_from_deployedcontainer(deployed_containers):\n",
    "    Allcontainers_running = run_linux_command('docker ps --format \"{{.ID}}: {{.Names}}\"').split('\\n')[:-1]\n",
    "    Allcontainers_nameid = []\n",
    "    for details in Allcontainers_running:\n",
    "        Allcontainers_nameid.append(details.split(':'))\n",
    "\n",
    "    deployed_containerid = []\n",
    "    for name in deployed_containers:\n",
    "        for record in Allcontainers_nameid:\n",
    "            if name in record[1]:\n",
    "                deployed_containerid.append(record[0])\n",
    "\n",
    "    return deployed_containerid  \n",
    "\n",
    "path = os.getcwd()\n",
    "run_linux_command('kubectl create -f bitcoin.yml')\n",
    "run_linux_command('kubectl create -f bitcoin2.yml')\n",
    "print 'Bitcoin deployment created.'\n",
    "\n",
    "run_linux_command('kubectl create -f deep_learning.yml')\n",
    "print 'Deep learning depoyment created. '\n",
    "\n",
    "#run_linux_command('kubectl create -f mysql-pv.yml')\n",
    "#run_linux_command('kubectl create -f mysql-service.yml')\n",
    "#run_linux_command('kubectl create -f mysql-deployment.yml')\n",
    "#print 'Mysql deployment created.'\n",
    "\n",
    "#run_linux_command('kubectl create -f stress.yml')\n",
    "#print 'Stress Ubuntu workload created.'\n",
    "\n",
    "print 'Waiting for 30 sec to let workload run.'\n",
    "time.sleep(30)  #Wait for 30 sec to receive bitcoin task from main server\n",
    "\n",
    "#print subprocess.check_output([\"docker\", \"ps\", \"--format\", \"table {{.ID}} \\t {{.Names}} \\t {{.Status}}\"])\n",
    "#os.system('git clone https://github.com/cloudviz/agentless-system-crawler')\n",
    "current_working_dir = os.getcwd()\n",
    "#new_path = current_working_dir+'/agentless-system-crawler'\n",
    "#os.chdir(new_path)\n",
    "#os.system('docker build -t crawler .')\n",
    "#Image_build =  subprocess.check_output(['docker', 'image', 'ls'])\n",
    "#for i,crawl in enumerate(Image_build.split('\\n')):\n",
    "#    if i == 0:\n",
    "#        print crawl\n",
    "#    if 'crawler' in crawl:\n",
    "#        print crawl\n",
    "\n",
    "os.chdir(path)\n",
    "deployed_containers = get_container_names(os.getcwd())\n",
    "deployed_containerid = containerid_from_deployedcontainer(deployed_containers)\n",
    "        \n",
    "crawler_command = ['docker', 'run', '--rm', '--privileged', '--net=host', '--pid=host', '-v', '/cgroup:/cgroup:ro', '-v', '/sys/fs/cgroup:/sys/fs/cgroup:ro', '-v', '/var/lib/docker:/var/lib/docker:ro', '-v', '/var/run/docker.sock:/var/run/docker.sock', '-v', '-it', 'crawler', '--crawlmode', 'OUTCONTAINER', '--features', 'cpu,metric,process,connection', '--crawlContainers', ','.join(deployed_containerid)]\n",
    "crawler_output = subprocess.check_output(crawler_command)\n",
    "crawler_output = crawler_output.split('\\n')\n",
    "\n",
    "os.chdir(current_working_dir)\n",
    "crawler_record_linestart = []\n",
    "containers_scanned = []\n",
    "dict_containers_nameid = {}\n",
    "\n",
    "for i,records in enumerate(crawler_output):\n",
    "    if records.startswith('metadata'):\n",
    "        crawler_record_linestart.append(i)\n",
    "        container_id =  ast.literal_eval(records.split('\\t')[2:][0])['container_short_id']\n",
    "        containers_scanned.append(container_id)\n",
    "        dict_containers_nameid[container_id] = ast.literal_eval(records.split('\\t')[2:][0])['container_name']\n",
    "\n",
    "dict_containers_nameid = remove_kubernetes_administrative_containers(dict_containers_nameid)\n",
    "print 'Containers scanned are : ',dict_containers_nameid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the informaton collected for crawler container itself\n",
    "try:\n",
    "    for i,records in enumerate(crawler_output):\n",
    "        if records.startswith('metadata'):\n",
    "            metadata_dict = ast.literal_eval(records.split('\\t')[2:][0])\n",
    "            container_image =  metadata_dict['docker_image_short_name']\n",
    "            if 'crawler' in container_image:\n",
    "                crawler_container = metadata_dict['container_short_id']\n",
    "                crawler_container_index = containers_scanned.index(crawler_container)\n",
    "                containers_scanned.remove(crawler_container)\n",
    "                del dict_containers_nameid[crawler_container]\n",
    "    try:\n",
    "        del crawler_output[crawler_record_linestart[crawler_container_index] : crawler_record_linestart[crawler_container_index+1]]\n",
    "\n",
    "    except:\n",
    "        del crawler_output[crawler_record_linestart[crawler_container_index] : len(crawler_output)]\n",
    "\n",
    "\n",
    "    print 'Length of collected crawler records:',len(crawler_output)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 7, 13], ['fe26b8b8e242', '5c368c1f7c4f', 'fdd276453842'])\n"
     ]
    }
   ],
   "source": [
    "crawler_record_linestart = []\n",
    "containers_scanned = []\n",
    "\n",
    "for i,records in enumerate(crawler_output):\n",
    "    if records.startswith('metadata'):\n",
    "        crawler_record_linestart.append(i)\n",
    "        container_id =  ast.literal_eval(records.split('\\t')[2:][0])['container_short_id']\n",
    "        containers_scanned.append(container_id)\n",
    "        \n",
    "print(crawler_record_linestart,containers_scanned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the CPU, process, metric, connection feature\n",
    "dict_container_crawled = {}\n",
    "cpu_names = []\n",
    "process_names = []\n",
    "connection_names = []\n",
    "metric_names = []\n",
    "for line_index,line_num,container_id in zip(range(len(crawler_record_linestart)),crawler_record_linestart,containers_scanned):\n",
    "    for i in range(line_num+1,len(crawler_output)):\n",
    "        try:\n",
    "            if i >= crawler_record_linestart[line_index+1]:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        if crawler_output[i].startswith('cpu'):\n",
    "            cpu_names.append(crawler_output[i].split('\\t')[2]) \n",
    "        \n",
    "        if crawler_output[i].startswith('process'):    \n",
    "            process_names.append(crawler_output[i].split('\\t')[1])\n",
    "            \n",
    "        if crawler_output[i].startswith('metric'):    \n",
    "            metric_names.append(crawler_output[i].split('\\t')[2])\n",
    "            \n",
    "        if crawler_output[i].startswith('connection'):    \n",
    "            connection_names.append(crawler_output[i].split('\\t')[1])\n",
    "    \n",
    "    dict_container_crawled[container_id+'_cpu'] = cpu_names\n",
    "    cpu_names = []\n",
    "    dict_container_crawled[container_id+'_process'] = process_names\n",
    "    process_names = []\n",
    "    dict_container_crawled[container_id+'_metric'] = metric_names\n",
    "    metric_names = []\n",
    "    dict_container_crawled[container_id+'_connection'] = connection_names\n",
    "    connection_names = []\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5c368c1f7c4f_connection': [],\n",
       " '5c368c1f7c4f_cpu': ['{\"cpu_idle\":39.122984,\"cpu_nice\":0.0,\"cpu_user\":57.65016027960673,\"cpu_wait\":0.0,\"cpu_system\":3.226855720393263,\"cpu_interrupt\":0.0,\"cpu_steal\":0.0,\"cpu_util\":60.877016}'],\n",
       " '5c368c1f7c4f_metric': ['{\"cpupct\":49.9,\"mempct\":32.12,\"pname\":\"python\",\"pid\":1,\"read\":162164736,\"rss\":2688700416,\"status\":\"sleeping\",\"user\":\"root\",\"vms\":4005437440,\"write\":23224320}',\n",
       "  '{\"cpupct\":0.0,\"mempct\":0.36,\"pname\":\"python2.7\",\"pid\":340,\"read\":0,\"rss\":30208000,\"status\":\"running\",\"user\":\"root\",\"vms\":151310336,\"write\":0}'],\n",
       " '5c368c1f7c4f_process': ['\"python/1\"', '\"python2.7/342\"'],\n",
       " 'fdd276453842_connection': [],\n",
       " 'fdd276453842_cpu': ['{\"cpu_idle\":100.0,\"cpu_nice\":0.0,\"cpu_user\":0.0,\"cpu_wait\":0.0,\"cpu_system\":0.0,\"cpu_interrupt\":0.0,\"cpu_steal\":0.0,\"cpu_util\":0.0}'],\n",
       " 'fdd276453842_metric': ['{\"cpupct\":0.0,\"mempct\":0.0,\"pname\":\"pause\",\"pid\":1,\"read\":0,\"rss\":4096,\"status\":\"sleeping\",\"user\":\"0\",\"vms\":1048576,\"write\":0}',\n",
       "  '{\"cpupct\":0.0,\"mempct\":0.36,\"pname\":\"python2.7\",\"pid\":20,\"read\":0,\"rss\":29794304,\"status\":\"running\",\"user\":\"0\",\"vms\":144822272,\"write\":0}'],\n",
       " 'fdd276453842_process': ['\"pause/1\"', '\"python2.7/22\"'],\n",
       " 'fe26b8b8e242_connection': ['\"1/172.17.0.2/54112\"'],\n",
       " 'fe26b8b8e242_cpu': ['{\"cpu_idle\":0,\"cpu_nice\":0.0,\"cpu_user\":141.8459027932783,\"cpu_wait\":0.0,\"cpu_system\":0.19741020672169476,\"cpu_interrupt\":0.0,\"cpu_steal\":0.0,\"cpu_util\":142.04331299999998}'],\n",
       " 'fe26b8b8e242_metric': ['{\"cpupct\":139.6,\"mempct\":0.07,\"pname\":\"xmrig\",\"pid\":1,\"read\":1019904,\"rss\":5681152,\"status\":\"sleeping\",\"user\":\"100\",\"vms\":10416128,\"write\":0}',\n",
       "  '{\"cpupct\":0.0,\"mempct\":0.36,\"pname\":\"python2.7\",\"pid\":30,\"read\":0,\"rss\":29794304,\"status\":\"running\",\"user\":\"0\",\"vms\":144822272,\"write\":0}'],\n",
       " 'fe26b8b8e242_process': ['\"xmrig/1\"', '\"python2.7/32\"']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_container_crawled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144.4\n",
      "65.04\n",
      "Alert! container: 5c368c1f7c4f might be suspicious\n",
      "The details are: \n",
      "CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES\n",
      "5c368c1f7c4f        5026a6d335fa        \"nohup python DEN/DE…\"   14 minutes ago      Up 14 minutes                           k8s_deep-learning-dynamic_deeplearning-deployment-7c7c877959-st4b9_default_9b7bdf87-a791-11e8-a186-025271d58932_0\n",
      "\n",
      "Alert! container: fe26b8b8e242 might be suspicious\n",
      "The details are: \n",
      "CONTAINER ID        IMAGE                COMMAND                  CREATED             STATUS              PORTS               NAMES\n",
      "fe26b8b8e242        bitnn/alpine-xmrig   \"./xmrig -o stratum+…\"   14 minutes ago      Up 14 minutes                           k8s_monero-cpuminer-deployment_monero-cpuminer-deployment-bb5fb4dd9-kn92t_default_9b32d7ef-a791-11e8-a186-025271d58932_0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#processing CPU feature for kubernetes with dockersats\n",
    "dict_container_CPUusage = {}\n",
    "containers_scanned = dict_containers_nameid.keys()\n",
    "for container_name in containers_scanned:\n",
    "    CPU_sample_value = []\n",
    "    for i in [0,1,2,3,4]:\n",
    "        command = \"docker stats \" + container_name + \" --no-stream | awk 'FNR == 2 {print $3}'\"\n",
    "        text = run_linux_command(command)\n",
    "        CPU_sample_value.append(float(text.split('%')[0]))\n",
    "        time.sleep(0.1)\n",
    "    print max(CPU_sample_value)\n",
    "    dict_container_CPUusage[container_name] = max(CPU_sample_value)\n",
    "    \n",
    "suspicious_containers = []\n",
    "\n",
    "for dict_key in  sorted(dict_container_CPUusage.keys()):\n",
    "    if dict_container_CPUusage[dict_key] > 30:\n",
    "        print 'Alert! container: {0} might be suspicious'.format(dict_key)\n",
    "        print 'The details are: \\n', run_linux_command('docker ps --filter id='+dict_key)\n",
    "        suspicious_containers.append(dict_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: the sets module is deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sets import Set\n",
    "if len(Set(dict_container_CPUusage.keys()) - Set(dict_containers_nameid.keys())) == 0: \n",
    "    for key in dict_containers_nameid.keys():\n",
    "        dict_container_CPUusage[dict_containers_nameid[key]] = dict_container_CPUusage[key]\n",
    "        del dict_container_CPUusage[key]\n",
    "\n",
    "    import matplotlib.pyplot as plt, mpld3\n",
    "    lists = sorted(dict_container_CPUusage.items()) # sorted by key, return a list of tuples\n",
    "\n",
    "    x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "    ax =plt.figure().add_subplot(111)\n",
    "    plt.plot(x,y, 'ks-', mec='w', mew=5, ms=20)\n",
    "    plt.xticks(x, sorted(dict_container_CPUusage.keys()), rotation=90)\n",
    "    ax.set_xlabel('Containers')\n",
    "    ax.set_ylabel('CPU Usage (%)')\n",
    "    plt.savefig('CPU_plot.pdf')\n",
    "    plt.show()\n",
    "    #mpld3.display()\n",
    "else:\n",
    "    print 'Something is Wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    CPU Usage  Syscalls\n",
      "k8s_deep-learning-dynamic_deeplearning-deployme...      65.04       0.0\n",
      "k8s_monero-cpuminer-deployment_monero-cpuminer-...     144.40       0.0\n"
     ]
    }
   ],
   "source": [
    "#Showing the result in table\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame.from_dict(dict_container_CPUusage, orient='index' )\n",
    "df.columns = ['CPU Usage']\n",
    "df['Syscalls'] = np.zeros((len(dict_container_CPUusage.keys()),1))\n",
    "\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileinput\n",
    "import re\n",
    "import glob\n",
    "\n",
    "def grep(PAT, FILES):\n",
    "    returned_grep = []\n",
    "    for line in fileinput.input(glob.glob(FILES)):\n",
    "        if re.search(PAT, line):\n",
    "             returned_grep.append(line)\n",
    "    return returned_grep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_containers = ['5c368c1f7c4f', '6fc471847760', 'f185c48b505b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syscalls collection started.\n",
      "Syscalls collection completed.\n",
      "Container 5c368c1f7c4f is found real and normal, alert was false.\n",
      "The details are: \n",
      "CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES\n",
      "5c368c1f7c4f        5026a6d335fa        \"nohup python DEN/DE…\"   About an hour ago   Up About an hour                        k8s_deep-learning-dynamic_deeplearning-deployment-7c7c877959-st4b9_default_9b7bdf87-a791-11e8-a186-025271d58932_0\n",
      "\n",
      "Syscalls collection started.\n",
      "Syscalls collection completed.\n",
      "Container 6fc471847760 is anomaly, so disabling.\n",
      "The details are: \n",
      "CONTAINER ID        IMAGE                COMMAND                  CREATED             STATUS              PORTS               NAMES\n",
      "6fc471847760        bitnn/alpine-xmrig   \"./xmrig -o stratum+…\"   24 minutes ago      Up 24 minutes                           k8s_monero-cpuminer-deployment_monero-cpuminer-deployment-bb5fb4dd9-mc8qt_default_7f374133-a795-11e8-a186-025271d58932_0\n",
      "\n",
      "Disabled and deleted.\n",
      "Syscalls collection started.\n",
      "Syscalls collection completed.\n",
      "Container f185c48b505b is anomaly, so disabling.\n",
      "The details are: \n",
      "CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES\n",
      "f185c48b505b        25ec25df1845        \"docker-entrypoint.s…\"   14 minutes ago      Up 14 minutes                           k8s_cassandra_cassandra-7f9744d895-tv72x_default_0685d680-a797-11e8-a186-025271d58932_0\n",
      "\n",
      "Disabled and deleted.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from multiprocessing import Process, Pipe\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "def collect_syscalls(mpPipe):\n",
    "    # This will take a long time\n",
    "    print \"Syscalls collection started.\"\n",
    "    with open('syscall_run_cmd.sh','w') as fl:\n",
    "        fl.write(\"perf record -e 'raw_syscalls:sys_enter' -g --exclude-perf -a  --call-graph dwarf,4096 -o sys_call_collected -G kubepods/\" + qosClass_type + \"/pod\"+poduid+ \"/\"+ container_longID)\n",
    "    os.system('sh syscall_run_cmd.sh')\n",
    "    mpPipe.send(\"Done\")\n",
    "    \n",
    "def n_grams_split(input_list, len_ngram):\n",
    "        n_gram_seq = []\n",
    "        for i in range(len(input_list)-len_ngram+1):\n",
    "            n_gram_seq.append(input_list[i:i+len_ngram])\n",
    "        return(n_gram_seq)\n",
    "    \n",
    "for container_name in suspicious_containers:\n",
    "    container_longID = subprocess.check_output(['docker', 'inspect', '--format=\"{{.Id}}\"', container_name])\n",
    "    container_longID = ast.literal_eval(container_longID[:-1])\n",
    "    pod_name = run_linux_command(\"kubectl get pod -o jsonpath='{range .items[?(@.status.containerStatuses[].containerID==\\\"docker://\" + container_longID +\"\\\")]}{.metadata.name}{end}'\")\n",
    "    run_linux_command('kubectl get pods '+pod_name + ' -o yaml > pod_yamlfile')\n",
    "    qosClass_type = grep('qosClass','pod_yamlfile')\n",
    "    poduid = grep('uid','pod_yamlfile')\n",
    "    os.system('rm -rf pod_yamlfile')\n",
    "    qosClass_type = ''.join(qosClass_type)\n",
    "    qosClass_type = qosClass_type.lower()\n",
    "    if 'besteffort' in qosClass_type:\n",
    "        qosClass_type = 'besteffort'\n",
    "    else:\n",
    "        qosClass_type = 'burstable'\n",
    "    poduid = poduid[1].split(':')[1].strip() \n",
    "    #with open('syscall_run_cmd.sh','w') as fl:\n",
    "    #    fl.write(\"perf record -e 'raw_syscalls:sys_enter' -g --exclude-perf -a  --call-graph dwarf,4096 -o sys_call_collected -G docker/\" + container_longID)\n",
    "    #os.system('sh syscall_run_cmd.sh')\n",
    "    parent_conn, child_conn = Pipe()\n",
    "    p = Process(target=collect_syscalls, args=(child_conn,))\n",
    "    p.start()\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        os.system('pkill -f perf')\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(5)\n",
    "    print \"Syscalls collection completed.\"\n",
    "    os.system('perf script -i sys_call_collected > sys_call_numbers')\n",
    "    \n",
    "    os.system('rm -rf syscall_run_cmd.sh')\n",
    "    os.system('rm -rf sys_call_collected')\n",
    "    \n",
    "    with open('sys_call_numbers', 'r') as fl:\n",
    "        content = fl.readlines()\n",
    "    os.system('rm -rf sys_call_numbers')\n",
    "    if len(content) > 5:\n",
    "        Syscalls_list = []\n",
    "        for i in range(len(content)):\n",
    "            try: \n",
    "                Syscalls_list.append((content[i].split('NR')[1]).split(' ')[1])\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        Syscalls_frame = pd.DataFrame(Syscalls_list)\n",
    "\n",
    "        #Creating the sequence from the dataframe\n",
    "        window_size = 10\n",
    "        Sequence_anomaly = Syscalls_frame[Syscalls_frame.columns[0]]\n",
    "        Sequence_anomaly = list(Sequence_anomaly)\n",
    "        #Sequence_normal = split_list(Sequence_normal,window_size)\n",
    "        Sequence_anomaly = n_grams_split(Sequence_anomaly,window_size)\n",
    "        anomaly_seq_frame = pd.DataFrame(Sequence_anomaly)\n",
    "        if len(Sequence_anomaly[-1]) < window_size:\n",
    "            anomaly_seq_frame = anomaly_seq_frame.drop(anomaly_seq_frame.index[len(anomaly_seq_frame)-1])\n",
    "            print 'Last anomaly sequence deleted.'\n",
    "        X = anomaly_seq_frame.values\n",
    "        grad_boost = pickle.load(open('grad_boosting.pkl', 'rb'))\n",
    "        Y_pred_gradboost = grad_boost.predict(X)\n",
    "\n",
    "        decision_tree = pickle.load(open('decision_tree.pkl', 'rb'))\n",
    "        Y_pred_decisiontree = grad_boost.predict(X)\n",
    "\n",
    "        match_count = 0\n",
    "        unmatch_count = 0\n",
    "        for i in range(len(X)):\n",
    "            if Y_pred_gradboost[i] == Y_pred_decisiontree[i]:\n",
    "                match_count+=1\n",
    "            else:\n",
    "                unmatch_count+=1\n",
    "\n",
    "        if 0.8*match_count > unmatch_count:\n",
    "            ML_prediction_correctness = True\n",
    "        else:\n",
    "            ML_prediction_correctness = False\n",
    "\n",
    "        anomaly_predict_sample_gradboost = list(Y_pred_gradboost).count(1)\n",
    "        normal_predict_sample_gradboost = list(Y_pred_gradboost).count(0)\n",
    "        anomaly_predict_sample_decisiontree = list(Y_pred_decisiontree).count(1)\n",
    "        normal_predict_sample_decisiontree = list(Y_pred_decisiontree).count(0)\n",
    "\n",
    "        if 0.6*anomaly_predict_sample_gradboost > normal_predict_sample_gradboost:\n",
    "            grad_boost = 'Anomaly'\n",
    "        else:\n",
    "            grad_boost = 'Normal'\n",
    "\n",
    "        if 0.6*anomaly_predict_sample_decisiontree > normal_predict_sample_decisiontree:\n",
    "            decision_tree = 'Anomaly'\n",
    "        else:\n",
    "            decision_tree = 'Normal'\n",
    "\n",
    "        if grad_boost == 'Anomaly' and decision_tree == 'Anomaly' and ML_prediction_correctness == True:\n",
    "            print 'Container {0} is anomaly, so disabling.'.format(container_name)\n",
    "            print 'The details are: \\n', run_linux_command('docker ps --filter id='+container_name)\n",
    "            long_container_id = run_linux_command('docker inspect --format=\"{{.Id}}\" '+ container_name)\n",
    "            long_container_id = long_container_id[:-1]\n",
    "            pod_name = run_linux_command(\"kubectl get pod -o jsonpath='{range .items[?(@.status.containerStatuses[].containerID==\\\"docker://\" + long_container_id +\"\\\")]}{.metadata.name}{end}'\")\n",
    "            run_linux_command('kubectl delete pods '+pod_name)\n",
    "            print 'Disabled and deleted.'\n",
    "        else:\n",
    "            print 'Container {0} is found real and normal, alert was false.'.format(container_name)\n",
    "            print 'The details are: \\n', run_linux_command('docker ps --filter id='+container_name)\n",
    "    else:\n",
    "        print \"syscal not generated for pod {0}, so skipping it.\".format(pod_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_linux_command' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c5e511bdcc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mrun_linux_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kubectl delete -f bitcoin.yml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mrun_linux_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kubectl delete -f bitcoin2.yml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mrun_linux_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kubectl delete -f deep_learning.yml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mrun_linux_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kubectl delete -f mysql-pv.yml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mrun_linux_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kubectl delete -f mysql-service.yml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_linux_command' is not defined"
     ]
    }
   ],
   "source": [
    "print run_linux_command('kubectl delete -f bitcoin.yml')\n",
    "print run_linux_command('kubectl delete -f bitcoin2.yml')\n",
    "print run_linux_command('kubectl delete -f deep_learning.yml')\n",
    "print run_linux_command('kubectl delete -f mysql-pv.yml')\n",
    "print run_linux_command('kubectl delete -f mysql-service.yml')\n",
    "print run_linux_command('kubectl delete -f mysql-deployment.yml')\n",
    "print run_linux_command('kubectl delete -f stress.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
