{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql container created.\n",
      "Deep Learning Container created.\n",
      "Bitcoin container created. \n",
      "Waiting for 30 sec to let workload run.\n",
      "CONTAINER ID         NAMES                                                                                                                 STATUS\n",
      "a0aa5edf50e9         bitcoin2                                                                                                              Up 30 seconds\n",
      "19e454512699         bitcoin                                                                                                               Up 30 seconds\n",
      "d9cfd9f294ef         deep-learning                                                                                                         Up 31 seconds\n",
      "95db6d2d7885         some-mysql                                                                                                            Up 32 seconds\n",
      "688d8e4f5482         k8s_kubernetes-dashboard_kubernetes-dashboard-5498ccf677-2d9lp_kube-system_bb401221-8adb-11e8-b9e9-064c1c0f580b_0     Up 19 hours\n",
      "f61ea0f00176         k8s_POD_kubernetes-dashboard-5498ccf677-2d9lp_kube-system_bb401221-8adb-11e8-b9e9-064c1c0f580b_0                      Up 19 hours\n",
      "4e4449e96519         k8s_etcd_etcd-minikube_kube-system_6874a22698eb6854338a32cb9c494bc8_9                                                 Up 26 hours\n",
      "371e35314b62         k8s_kube-apiserver_kube-apiserver-minikube_kube-system_4b42b74481e4a56a4d1404361a991ee2_2                             Up 26 hours\n",
      "259d1775e9a3         k8s_kube-scheduler_kube-scheduler-minikube_kube-system_31cf0ccbee286239d451edb6fb511513_4                             Up 27 hours\n",
      "8b6779a7c84a         k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_4c229eb47878327ba1a24f38b8334f2d_2           Up 27 hours\n",
      "36218c77053b         k8s_metrics-server_metrics-server-6fbfb84cdd-67h5s_kube-system_27610dc2-89e8-11e8-be32-064c1c0f580b_0                 Up 2 days\n",
      "ea9619606049         k8s_POD_metrics-server-6fbfb84cdd-67h5s_kube-system_27610dc2-89e8-11e8-be32-064c1c0f580b_0                            Up 2 days\n",
      "3a878cdeafda         k8s_sidecar_kube-dns-86f4d74b45-9lxf2_kube-system_b54c7312-8478-11e8-a382-064c1c0f580b_2                              Up 2 days\n",
      "e620b9cd25ac         k8s_dnsmasq_kube-dns-86f4d74b45-9lxf2_kube-system_b54c7312-8478-11e8-a382-064c1c0f580b_2                              Up 2 days\n",
      "6d5a069479d7         k8s_kube-proxy_kube-proxy-4rjdt_kube-system_b586403b-8478-11e8-a382-064c1c0f580b_2                                    Up 2 days\n",
      "906340454dae         k8s_kubedns_kube-dns-86f4d74b45-9lxf2_kube-system_b54c7312-8478-11e8-a382-064c1c0f580b_2                              Up 2 days\n",
      "21a7e5ef81fe         k8s_POD_kube-dns-86f4d74b45-9lxf2_kube-system_b54c7312-8478-11e8-a382-064c1c0f580b_2                                  Up 2 days\n",
      "d3c8ea784f6f         k8s_POD_kube-proxy-4rjdt_kube-system_b586403b-8478-11e8-a382-064c1c0f580b_2                                           Up 2 days\n",
      "60772d286641         k8s_tiller_tiller-deploy-f9b8476d-9pxsz_kube-system_01790b32-851f-11e8-a382-064c1c0f580b_2                            Up 2 days\n",
      "d62666a5f3e9         k8s_POD_tiller-deploy-f9b8476d-9pxsz_kube-system_01790b32-851f-11e8-a382-064c1c0f580b_2                               Up 2 days\n",
      "8885e6098cc6         k8s_storage-provisioner_storage-provisioner_kube-system_b693ddb3-8478-11e8-a382-064c1c0f580b_2                        Up 2 days\n",
      "2d7cf4b32baf         k8s_POD_storage-provisioner_kube-system_b693ddb3-8478-11e8-a382-064c1c0f580b_2                                        Up 2 days\n",
      "ad36740fc253         k8s_POD_etcd-minikube_kube-system_6874a22698eb6854338a32cb9c494bc8_0                                                  Up 2 days\n",
      "04a9c64bb1ab         k8s_POD_kube-controller-manager-minikube_kube-system_4c229eb47878327ba1a24f38b8334f2d_0                               Up 2 days\n",
      "99fae6f70b5f         k8s_POD_kube-apiserver-minikube_kube-system_4b42b74481e4a56a4d1404361a991ee2_0                                        Up 2 days\n",
      "0f6f897b8770         k8s_POD_kube-scheduler-minikube_kube-system_31cf0ccbee286239d451edb6fb511513_2                                        Up 2 days\n",
      "77f5d6498a22         k8s_kube-addon-manager_kube-addon-manager-minikube_kube-system_3afaf06535cc3b85be93c31632b765da_2                     Up 2 days\n",
      "109165d8c2ef         k8s_POD_kube-addon-manager-minikube_kube-system_3afaf06535cc3b85be93c31632b765da_2                                    Up 2 days\n",
      "d967bf7fef94         ubuntu1                                                                                                               Up 5 hours\n",
      "f15633975891         eloquent_davinci                                                                                                      Up 2 weeks\n",
      "\n",
      "REPOSITORY                                      TAG                 IMAGE ID            CREATED             SIZE\n",
      "crawler                                         latest              e4dbced03ba5        4 hours ago         947MB\n",
      "([0, 468, 476, 481, 487, 495, 500, 503, 570, 649, 654, 660, 668, 671, 675, 683, 693, 701, 704, 707, 712, 715, 719, 722, 725, 728, 731, 734, 738, 741, 749], ['6e145d662f7c', 'a0aa5edf50e9', '19e454512699', 'd9cfd9f294ef', '95db6d2d7885', '688d8e4f5482', 'f61ea0f00176', '4e4449e96519', '371e35314b62', '259d1775e9a3', '8b6779a7c84a', '36218c77053b', 'ea9619606049', '3a878cdeafda', 'e620b9cd25ac', '6d5a069479d7', '906340454dae', '21a7e5ef81fe', 'd3c8ea784f6f', '60772d286641', 'd62666a5f3e9', '8885e6098cc6', '2d7cf4b32baf', 'ad36740fc253', '04a9c64bb1ab', '99fae6f70b5f', '0f6f897b8770', '77f5d6498a22', '109165d8c2ef', 'd967bf7fef94', 'f15633975891'])\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "\n",
    "os.system('docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql')\n",
    "print 'mysql container created.'\n",
    "\n",
    "os.system('docker run -d --name deep-learning rupeshrajkarn/deeplearning:DEN python DEN/DEN_run.py')\n",
    "print 'Deep Learning Container created.'\n",
    "\n",
    "os.system('docker run  -d --name bitcoin --restart unless-stopped --read-only -m 50M  bitnn/alpine-xmrig -o stratum+tcp://xmr.crypto-pool.fr:3333 -u 41e2vPcVux9NNeTfWe8TLK2UWxCXJvNyCQtNb69YEexdNs711jEaDRXWbwaVe4vUMveKAzAiA4j8xgUi29TpKXpm3zKTUYo -p x -k --donate-level=1')\n",
    "os.system('docker run -d --name bitcoin2 cpuminer-multi --algo qubit --url stratum+tcp://digihash.co:3012 --user DJMczFzdq2NeBPhBxrFbMJxg98mgWeRWyo --pass anything --threads 3')\n",
    "print 'Bitcoin container created. '\n",
    "\n",
    "os.system('docker run -d --name=ubuntu_stressed rupeshrajkarn/stress_ubuntu bash -c \"stress --cpu 8 --timeout 2000\"')\n",
    "print 'Ubuntu container created.'\n",
    "print 'Waiting for 30 sec to let workload run.'\n",
    "time.sleep(30)\n",
    "\n",
    "print subprocess.check_output([\"docker\", \"ps\", \"--format\", \"table {{.ID}} \\t {{.Names}} \\t {{.Status}}\"])\n",
    "os.system('git clone https://github.com/cloudviz/agentless-system-crawler')\n",
    "current_working_dir = os.getcwd()\n",
    "new_path = current_working_dir+'/agentless-system-crawler'\n",
    "os.chdir(new_path)\n",
    "os.system('docker build -t crawler .')\n",
    "Image_build =  subprocess.check_output(['docker', 'image', 'ls'])\n",
    "for i,crawl in enumerate(Image_build.split('\\n')):\n",
    "    if i == 0:\n",
    "        print crawl\n",
    "    if 'crawler' in crawl:\n",
    "        print crawl\n",
    "        \n",
    "crawler_command = ['docker', 'run', '--rm', '--privileged', '--net=host', '--pid=host', '-v', '/cgroup:/cgroup:ro', '-v', '/sys/fs/cgroup:/sys/fs/cgroup:ro', '-v', '/var/lib/docker:/var/lib/docker:ro', '-v', '/var/run/docker.sock:/var/run/docker.sock', '-v', '-it', 'crawler', '--crawlmode', 'OUTCONTAINER', '--features', 'cpu,metric,process,connection']\n",
    "crawler_output = subprocess.check_output(crawler_command)\n",
    "crawler_output = crawler_output.split('\\n')\n",
    "\n",
    "os.chdir(current_working_dir)\n",
    "crawler_record_linestart = []\n",
    "containers_scanned = []\n",
    "\n",
    "for i,records in enumerate(crawler_output):\n",
    "    if records.startswith('metadata'):\n",
    "        crawler_record_linestart.append(i)\n",
    "        container_id =  ast.literal_eval(records.split('\\t')[2:][0])['container_short_id']\n",
    "        containers_scanned.append(container_id)\n",
    "        \n",
    "print(crawler_record_linestart,containers_scanned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of collected crawler records: 286\n"
     ]
    }
   ],
   "source": [
    "#Delete the informaton collected for crawler container itself\n",
    "for i,records in enumerate(crawler_output):\n",
    "    if records.startswith('metadata'):\n",
    "        metadata_dict = ast.literal_eval(records.split('\\t')[2:][0])\n",
    "        container_image =  metadata_dict['docker_image_short_name']\n",
    "        if 'crawler' in container_image:\n",
    "            crawler_container = metadata_dict['container_short_id']\n",
    "            crawler_container_index = containers_scanned.index(crawler_container)\n",
    "            containers_scanned.remove(crawler_container)\n",
    "try:\n",
    "    del crawler_output[crawler_record_linestart[crawler_container_index] : crawler_record_linestart[crawler_container_index+1]]\n",
    "        \n",
    "except:\n",
    "    del crawler_output[crawler_record_linestart[crawler_container_index] : len(crawler_output)]\n",
    "        \n",
    "            \n",
    "print 'Length of collected crawler records:',len(crawler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 8, 13, 19, 27, 32, 35, 102, 181, 186, 192, 200, 203, 207, 215, 225, 233, 236, 239, 244, 247, 251, 254, 257, 260, 263, 266, 270, 273, 281], ['a0aa5edf50e9', '19e454512699', 'd9cfd9f294ef', '95db6d2d7885', '688d8e4f5482', 'f61ea0f00176', '4e4449e96519', '371e35314b62', '259d1775e9a3', '8b6779a7c84a', '36218c77053b', 'ea9619606049', '3a878cdeafda', 'e620b9cd25ac', '6d5a069479d7', '906340454dae', '21a7e5ef81fe', 'd3c8ea784f6f', '60772d286641', 'd62666a5f3e9', '8885e6098cc6', '2d7cf4b32baf', 'ad36740fc253', '04a9c64bb1ab', '99fae6f70b5f', '0f6f897b8770', '77f5d6498a22', '109165d8c2ef', 'd967bf7fef94', 'f15633975891'])\n"
     ]
    }
   ],
   "source": [
    "crawler_record_linestart = []\n",
    "containers_scanned = []\n",
    "\n",
    "for i,records in enumerate(crawler_output):\n",
    "    if records.startswith('metadata'):\n",
    "        crawler_record_linestart.append(i)\n",
    "        container_id =  ast.literal_eval(records.split('\\t')[2:][0])['container_short_id']\n",
    "        containers_scanned.append(container_id)\n",
    "        \n",
    "print(crawler_record_linestart,containers_scanned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_container_crawled = {}\n",
    "cpu_names = []\n",
    "process_names = []\n",
    "connection_names = []\n",
    "metric_names = []\n",
    "for line_index,line_num,container_id in zip(range(len(crawler_record_linestart)),crawler_record_linestart,containers_scanned):\n",
    "    for i in range(line_num+1,len(crawler_output)):\n",
    "        try:\n",
    "            if i >= crawler_record_linestart[line_index+1]:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        if crawler_output[i].startswith('cpu'):\n",
    "            cpu_names.append(crawler_output[i].split('\\t')[2]) \n",
    "        \n",
    "        if crawler_output[i].startswith('process'):    \n",
    "            process_names.append(crawler_output[i].split('\\t')[1])\n",
    "            \n",
    "        if crawler_output[i].startswith('metric'):    \n",
    "            metric_names.append(crawler_output[i].split('\\t')[2])\n",
    "            \n",
    "        if crawler_output[i].startswith('connection'):    \n",
    "            connection_names.append(crawler_output[i].split('\\t')[1])\n",
    "    \n",
    "    dict_container_crawled[container_id+'_cpu'] = cpu_names\n",
    "    cpu_names = []\n",
    "    dict_container_crawled[container_id+'_process'] = process_names\n",
    "    process_names = []\n",
    "    dict_container_crawled[container_id+'_metric'] = metric_names\n",
    "    metric_names = []\n",
    "    dict_container_crawled[container_id+'_connection'] = connection_names\n",
    "    connection_names = []\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert! container: a0aa5edf50e9 might be suspicious\n",
      "Alert! container: d9cfd9f294ef might be suspicious\n"
     ]
    }
   ],
   "source": [
    "#processing CPU feature with crawler\n",
    "def run_linux_command(command):\n",
    "    p = subprocess.Popen(command, universal_newlines=True, shell=True, \n",
    "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    text = p.stdout.read()\n",
    "    retcode = p.wait()\n",
    "    return text\n",
    "\n",
    "dict_container_crawled_keys = sorted(dict_container_crawled.keys())\n",
    "suspicious_containers = []\n",
    "\n",
    "for dict_key in  dict_container_crawled_keys:\n",
    "    if 'cpu' in dict_key:\n",
    "        for i in range(len(dict_container_crawled[dict_key])):\n",
    "            cpu_idle_percent = ast.literal_eval(dict_container_crawled[dict_key][i])['cpu_idle']\n",
    "            if cpu_idle_percent < 70:\n",
    "                suspicious_container = dict_key.split('_')[0]\n",
    "                print 'Alert! container: {0} might be suspicious'.format(suspicious_container)\n",
    "                suspicious_containers.append(suspicious_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syscalls collection started.\n",
      "Syscalls collection completed.\n",
      "Container d9cfd9f294ef is found real and normal, alert was false.\n",
      "The details are: \n",
      "CONTAINER ID        IMAGE                            COMMAND                  CREATED             STATUS              PORTS                NAMES\n",
      "d9cfd9f294ef        rupeshrajkarn/deeplearning:DEN   \"python DEN/DEN_run.…\"   2 minutes ago       Up 2 minutes        6006/tcp, 8888/tcp   deep-learning\n",
      "\n",
      "Syscalls collection started.\n",
      "Syscalls collection completed.\n",
      "Container a0aa5edf50e9 is anomaly, so disabling.\n",
      "The details are: \n",
      "CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES\n",
      "a0aa5edf50e9        cpuminer-multi      \"./cpuminer --algo q…\"   3 minutes ago       Up 3 minutes                            bitcoin2\n",
      "\n",
      "Disabled and deleted.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from multiprocessing import Process, Pipe\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "import pdb\n",
    "\n",
    "def collect_syscalls(mpPipe):\n",
    "    # This will take a long time\n",
    "    print \"Syscalls collection started.\"\n",
    "    with open('syscall_run_cmd.sh','w') as fl:\n",
    "        fl.write(\"perf record -e 'raw_syscalls:sys_enter' -g --exclude-perf -a  --call-graph dwarf,4096 -o sys_call_collected -G docker/\" + container_longID)\n",
    "    os.system('sh syscall_run_cmd.sh')\n",
    "    print \"Syscalls collection completed.\"\n",
    "    mpPipe.send(\"Done\")\n",
    "    \n",
    "def n_grams_split(input_list, len_ngram):\n",
    "        n_gram_seq = []\n",
    "        for i in range(len(input_list)-len_ngram+1):\n",
    "            n_gram_seq.append(input_list[i:i+len_ngram])\n",
    "        return(n_gram_seq)\n",
    "    \n",
    "for container_name in suspicious_containers:\n",
    "    container_longID = subprocess.check_output(['docker', 'inspect', '--format=\"{{.Id}}\"', container_name])\n",
    "    container_longID = ast.literal_eval(container_longID[:-1])\n",
    "    #with open('syscall_run_cmd.sh','w') as fl:\n",
    "    #    fl.write(\"perf record -e 'raw_syscalls:sys_enter' -g --exclude-perf -a  --call-graph dwarf,4096 -o sys_call_collected -G docker/\" + container_longID)\n",
    "    #os.system('sh syscall_run_cmd.sh')\n",
    "    parent_conn, child_conn = Pipe()\n",
    "    p = Process(target=collect_syscalls, args=(child_conn,))\n",
    "    p.start()\n",
    "    time.sleep(10)\n",
    "    try:\n",
    "        os.system('pkill -f perf')\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(5)\n",
    "    os.system('perf script -i sys_call_collected > sys_call_numbers')\n",
    "    #pdb.set_trace()\n",
    "    os.system('rm -rf syscall_run_cmd.sh')\n",
    "    os.system('rm -rf sys_call_collected')\n",
    "    \n",
    "    with open('sys_call_numbers', 'r') as fl:\n",
    "        content = fl.readlines()\n",
    "\n",
    "    Syscalls_list = []\n",
    "    for i in range(len(content)):\n",
    "        try: \n",
    "            Syscalls_list.append((content[i].split('NR')[1]).split(' ')[1])\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    Syscalls_frame = pd.DataFrame(Syscalls_list)\n",
    "    os.system('rm -rf sys_call_numbers')\n",
    "\n",
    "    #Creating the sequence from the dataframe\n",
    "    window_size = 10\n",
    "    Sequence_anomaly = Syscalls_frame[Syscalls_frame.columns[0]]\n",
    "    Sequence_anomaly = list(Sequence_anomaly)\n",
    "    #Sequence_normal = split_list(Sequence_normal,window_size)\n",
    "    Sequence_anomaly = n_grams_split(Sequence_anomaly,window_size)\n",
    "    anomaly_seq_frame = pd.DataFrame(Sequence_anomaly)\n",
    "    if len(Sequence_anomaly[-1]) < window_size:\n",
    "        anomaly_seq_frame = anomaly_seq_frame.drop(anomaly_seq_frame.index[len(anomaly_seq_frame)-1])\n",
    "        print 'Last anomaly sequence deleted.'\n",
    "    X = anomaly_seq_frame.values\n",
    "    grad_boost = pickle.load(open('grad_boosting.pkl', 'rb'))\n",
    "    Y_pred_gradboost = grad_boost.predict(X)\n",
    "\n",
    "    decision_tree = pickle.load(open('decision_tree.pkl', 'rb'))\n",
    "    Y_pred_decisiontree = grad_boost.predict(X)\n",
    "\n",
    "    match_count = 0\n",
    "    unmatch_count = 0\n",
    "    for i in range(len(X)):\n",
    "        if Y_pred_gradboost[i] == Y_pred_decisiontree[i]:\n",
    "            match_count+=1\n",
    "        else:\n",
    "            unmatch_count+=1\n",
    "\n",
    "    if 0.8*match_count > unmatch_count:\n",
    "        ML_prediction_correctness = True\n",
    "    else:\n",
    "        ML_prediction_correctness = False\n",
    "\n",
    "    anomaly_predict_sample_gradboost = list(Y_pred_gradboost).count(1)\n",
    "    normal_predict_sample_gradboost = list(Y_pred_gradboost).count(0)\n",
    "    anomaly_predict_sample_decisiontree = list(Y_pred_decisiontree).count(1)\n",
    "    normal_predict_sample_decisiontree = list(Y_pred_decisiontree).count(0)\n",
    "\n",
    "    if 0.6*anomaly_predict_sample_gradboost > normal_predict_sample_gradboost:\n",
    "        grad_boost = 'Anomaly'\n",
    "    else:\n",
    "        grad_boost = 'Normal'\n",
    "\n",
    "    if 0.6*anomaly_predict_sample_decisiontree > normal_predict_sample_decisiontree:\n",
    "        decision_tree = 'Anomaly'\n",
    "    else:\n",
    "        decision_tree = 'Normal'\n",
    "\n",
    "    if grad_boost == 'Anomaly' and decision_tree == 'Anomaly' and ML_prediction_correctness == True:\n",
    "        print 'Container {0} is anomaly, so disabling.'.format(container_name)\n",
    "        print 'The details are: \\n', run_linux_command('docker ps --filter id='+container_name)\n",
    "        disable_command = 'docker stop '+ container_name\n",
    "        delete_command = 'docker rm ' + container_name\n",
    "        os.system(disable_command)\n",
    "        os.system(delete_command)\n",
    "        print 'Disabled and deleted.'\n",
    "    else:\n",
    "        print 'Container {0} is found real and normal, alert was false.'.format(container_name)\n",
    "        print 'The details are: \\n', run_linux_command('docker ps --filter id='+container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_predict_sample_gradboost,normal_predict_sample_gradboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
